<HTML>
<CENTER><A HREF = "http://lammps.sandia.gov">LAMMPS WWW Site</A> - <A HREF = "Manual.html">LAMMPS Documentation</A> - <A HREF = "Section_commands.html#comm">LAMMPS Commands</A> 
</CENTER>






<HR>

<H3>package command 
</H3>
<P><B>Syntax:</B>
</P>
<PRE>package style args 
</PRE>
<UL><LI>style = <I>gpu</I> or <I>cuda</I> or <I>omp</I> 

<LI>args = arguments specific to the style 

<PRE>  <I>gpu</I> args = mode first last split
    mode = force or force/neigh
    first = ID of first GPU to be used on each node
    last = ID of last GPU to be used on each node
    split = fraction of particles assigned to the GPU
  <I>cuda</I> args = to be determined
  <I>omp</I> args = Nthreads mode
    Nthreads = # of OpenMP threads to associate with each MPI process
    mode = force or force/neigh (optional) 
</PRE>

</UL>
<P><B>Examples:</B>
</P>
<PRE>package gpu force 0 0 1.0
package gpu force 0 0 0.75
package gpu force/neigh 0 0 1.0
package gpu force/neigh 0 1 -1.0
package omp * force/neigh
package omp 4 force 
</PRE>
<P><B>Description:</B>
</P>
<P>This command invokes package-specific settings.  Currently the
following packages use it: GPU, USER-CUDA, and USER-OMP.
</P>
<P>See <A HREF = "Section_accelerate.html">this section</A> of the manual for more
details about using these various packages for accelerating
a LAMMPS calculation.
</P>
<HR>

<P>The <I>gpu</I> style invokes options associated with the use of the GPU
package.  It allows you to select and initialize GPUs to be used for
acceleration via this package and configure how the GPU acceleration
is performed.  These settings are required in order to use any style
with GPU acceleration.
</P>
<P>The <I>mode</I> setting specifies where neighbor list calculations will be
performed.  If <I>mode</I> is force, neighbor list calculation is performed
on the CPU. If <I>mode</I> is force/neigh, neighbor list calculation is
performed on the GPU. GPU neighbor list calculation currently cannot
be used with a triclinic box. GPU neighbor list calculation currently
cannot be used with <A HREF = "pair_hybrid.html">hybrid</A> pair styles.  GPU
neighbor lists are not compatible with styles that are not
GPU-enabled.  When a non-GPU enabled style requires a neighbor list,
it will also be built using CPU routines. In these cases, it will
typically be more efficient to only use CPU neighbor list builds.
</P>
<P>The <I>first</I> and <I>last</I> settings specify the GPUs that will be used for
simulation.  On each node, the GPU IDs in the inclusive range from
<I>first</I> to <I>last</I> will be used.
</P>
<P>The <I>split</I> setting can be used for load balancing force calculation
work between CPU and GPU cores in GPU-enabled pair styles. If 0 <
<I>split</I> < 1.0, a fixed fraction of particles is offloaded to the GPU
while force calculation for the other particles occurs simulataneously
on the CPU. If <I>split</I><0, the optimal fraction (based on CPU and GPU
timings) is calculated every 25 timesteps. If <I>split</I> = 1.0, all force
calculations for GPU accelerated pair styles are performed on the
GPU. In this case, <A HREF = "pair_hybrid.html">hybrid</A>, <A HREF = "bond_style.html">bond</A>,
<A HREF = "angle_style.html">angle</A>, <A HREF = "dihedral_style.html">dihedral</A>,
<A HREF = "improper_style.html">improper</A>, and <A HREF = "kspace_style.html">long-range</A>
calculations can be performed on the CPU while the GPU is performing
force calculations for the GPU-enabled pair style.  If all CPU force
computations complete before the GPU, LAMMPS will block until the GPU
has finished before continuing the timestep.
</P>
<P>As an example, if you have two GPUs per node and 8 CPU cores per node,
and would like to run on 4 nodes (32 cores) with dynamic balancing of
force calculation across CPU and GPU cores, you could specify
</P>
<PRE>package gpu force/neigh 0 1 -1 
</PRE>
<P>In this case, all CPU cores and GPU devices on the nodes would be
utilized.  Each GPU device would be shared by 4 CPU cores. The CPU
cores would perform force calculations for some fraction of the
particles at the same time the GPUs performed force calculation for
the other particles.
</P>
<HR>

<P>The <I>cuda</I> style invokes options associated with the use of the
USER-CUDA package.  These still need to be documented.
</P>
<HR>

<P>The <I>omp</I> style invokes options associated with the use of the
USER-OMP package.
</P>
<P>The first argument allows to explicitly set the number of OpenMP
threads to be allocated for each MPI process.  For example, if your
system has nodes with dual quad-core processors, it has a total of 8
cores per node.  You could run MPI on 2 cores on each node (e.g. using
options for the mpirun command), and set the <I>Nthreads</I> setting to 4.
This would effectively use all 8 cores on each node.  Since each MPI
process would spawn 4 threads (one of which runs as part of the MPI
process itself).
</P>
<P>For performance reasons, you should not set <I>Nthreads</I> to more threads
than there are physical cores (per MPI task), but LAMMPS cannot check
for this.
</P>
<P>An <I>Nthreads</I> value of '*' instructs LAMMPS to use whatever is the
default for the given OpenMP environment. This is usually determined
via the <I>OMP_NUM_THREADS</I> environment variable or the compiler
runtime.  Please note that in most cases the default for OpenMP
capable compilers is to use one thread for each available CPU core
when <I>OMP_NUM_THREADS</I> is not set, which can lead to extremely bad
performance.
</P>
<P>Which combination of threads and MPI tasks gives the best performance
is difficult to predict and can depend on many components of your input.
Not all features of LAMMPS support OpenMP and the parallel efficiency
can be very different, too.
</P>
<P>The <I>mode</I> setting specifies where neighbor list calculations will be
multi-threaded as well.  If <I>mode</I> is force, neighbor list calculation
is performed in serial. If <I>mode</I> is force/neigh, a multi-threaded
neighbor list build is used. Using the force/neigh setting is almost
always faster and should produce idential neighbor lists at the
expense of using some more memory (neighbor list pages are always
allocated for all threads at the same time and each thread works on
its own pages).
</P>
<HR>

<P><B>Restrictions:</B>
</P>
<P>This command cannot be used after the simulation box is defined by a
<A HREF = "read_data.html">read_data</A> or <A HREF = "create_box.html">create_box</A> command.
</P>
<P>The cuda style of this command can only be invoked if LAMMPS was built
with the USER-CUDA package.  See the <A HREF = "Section_start.html#start_3">Making
LAMMPS</A> section for more info.  When using
styles in the USER-CUDA package, use of the "package cuda" command in
your input script is not required.
</P>
<P>The gpu style of this command can only be invoked if LAMMPS was built
with the GPU package.  See the <A HREF = "Section_start.html#start_3">Making
LAMMPS</A> section for more info.  When using
styles in the GPU package, use of the "package gpu" command in your
input script is currently required.
</P>
<P>The omp style of this command can only be invoked if LAMMPS was built
with the USER-OMP package.  See the <A HREF = "Section_start.html#start_3">Making
LAMMPS</A> section for more info.  When using
styles in the USER-OMP package, use of the "package omp" command in
your input script is not required.  See the information on default
settings below.
</P>
<P><B>Related commands:</B>
</P>
<P><A HREF = "suffix.html">suffix</A>
</P>
<P><B>Default:</B>
</P>
<P>If the "-sf omp" <A HREF = "Section_start.html#2_6">command-line switch</A> is used
then "package omp *" is also auto-invoked to specify default OMP
settings.
</P>
</HTML>
